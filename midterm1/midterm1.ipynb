{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOOKING AT EPOCH = 50, LEARNING_RATE = 0.1 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learningRate \u001b[38;5;129;01min\u001b[39;00m allPossibleLearning_rate:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m allPossibleEpochs:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m#error\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m         curMeanTrainLoss , curMeanValLoss \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearningRate\u001b[39m\u001b[38;5;124m'\u001b[39m: learningRate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeanTrainLoss\u001b[39m\u001b[38;5;124m'\u001b[39m: curMeanTrainLoss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeanValLoss\u001b[39m\u001b[38;5;124m'\u001b[39m: curMeanValLoss})\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#Print all of the final results for every pair of hyperparameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 76\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[0;34m(x_train, y_train, epoch, learningRate)\u001b[0m\n\u001b[1;32m     73\u001b[0m y_val_fold \u001b[38;5;241m=\u001b[39m y_train[val_index]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Find trainingloss and validation loss with given validation and training sets.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m trainLoss, valLoss \u001b[38;5;241m=\u001b[39m \u001b[43mperform_Logistic_Regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m fold_train_losses\u001b[38;5;241m.\u001b[39mappend(trainLoss)\n\u001b[1;32m     78\u001b[0m fold_val_losses\u001b[38;5;241m.\u001b[39mappend(valLoss)\n",
      "Cell \u001b[0;32mIn[23], line 47\u001b[0m, in \u001b[0;36mperform_Logistic_Regression\u001b[0;34m(xTrain, yTrain, xVal, yVal, epoch, learningRate)\u001b[0m\n\u001b[1;32m     44\u001b[0m     error \u001b[38;5;241m=\u001b[39m yTrain[i] \u001b[38;5;241m-\u001b[39m YHat\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weights)):\n\u001b[0;32m---> 47\u001b[0m         gradient \u001b[38;5;241m=\u001b[39m learningRate \u001b[38;5;241m*\u001b[39m error \u001b[38;5;241m*\u001b[39m \u001b[43mxRow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m         weights[j] \u001b[38;5;241m=\u001b[39m weights[j] \u001b[38;5;241m-\u001b[39m gradient\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# now we find losses at the end of the epoch after changing weights for every parameter\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "trainingArr = pd.read_csv('./train_midterm.csv')\n",
    "testArr = pd.read_csv('./test_midterm.csv')\n",
    "\n",
    "x_test = testArr.drop('label', axis=1).values\n",
    "y_test = testArr['label'].values  # Add .values here\n",
    "x_train = trainingArr.drop('label', axis=1).values\n",
    "y_train = trainingArr['label'].values\n",
    "\n",
    "allPossibleLearning_rate = [0.1, 0.01, 0.001]\n",
    "allPossibleEpochs = [50, 200, 500]\n",
    "\n",
    "# NEEDS TO ONLY RETURN LAST TRAIN LOSS AND VAL LOSS\n",
    "\n",
    "def findLossForDataSet(xData, yData, weights):\n",
    "    loss = 0\n",
    "    for i in range(len(xData)):\n",
    "        # Calculate prediction\n",
    "        wx = sum(weights[j] * xData[i][j] for j in range(len(weights)))\n",
    "        yHat = 1 / (1 + np.exp(-wx))\n",
    "        \n",
    "        # doing binary cross-entropy\n",
    "        loss += -(yData[i] * np.log(yHat + 1e-10) + (1 - yData[i]) * np.log(1 - yHat + 1e-10))\n",
    "    return loss / len(xData)\n",
    "\n",
    "def perform_Logistic_Regression(xTrain, yTrain, xVal, yVal, epoch, learningRate):\n",
    "    trainingLoss = []\n",
    "    valLoss = []\n",
    "    weights = [0] * len(x_test[0])\n",
    "    for ep in range(epoch):\n",
    "        for i in range(len(xTrain)):\n",
    "            xRow = xTrain[i]\n",
    "\n",
    "            # first finding the dot product of current weights and xRow\n",
    "            beforeSigmoidYHat = np.dot(weights, xRow) \n",
    "\n",
    "            # turning this variable above to the actualyHat by using the sigmoid function, this is necessary in logistic regression since we need to convert any number to a percentage\n",
    "            # this sigmoid function will convert any number (-inf, inf) -> [0,1]\n",
    "            YHat = 1 / (1 + np.exp(-beforeSigmoidYHat))\n",
    "            error = yTrain[i] - YHat\n",
    "\n",
    "            for j in range(len(weights)):\n",
    "                gradient = learningRate * error * xRow[j]\n",
    "                weights[j] = weights[j] - gradient\n",
    "\n",
    "        # now we find losses at the end of the epoch after changing weights for every parameter\n",
    "        train_loss = findLossForDataSet(xTrain, yTrain, weights)\n",
    "        val_loss = findLossForDataSet(xVal, yVal, weights)\n",
    "        trainingLoss.append(train_loss)\n",
    "        valLoss.append(val_loss)\n",
    "    \n",
    "    # returning only the last training and validation loss\n",
    "    return trainingLoss[-1], valLoss[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perform_cross_validation(x_train, y_train, epoch, learningRate):\n",
    "    print(f\"\\nLOOKING AT EPOCH = {epoch}, LEARNING_RATE = {learningRate} \\n\")\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    \n",
    "    for (train_index, val_index) in kf.split(x_train):\n",
    "        x_train_fold = x_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        x_val_fold = x_train[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Find trainingloss and validation loss with given validation and training sets.\n",
    "        trainLoss, valLoss = perform_Logistic_Regression(x_train_fold, y_train_fold, x_val_fold, y_val_fold, epoch, learningRate)\n",
    "        fold_train_losses.append(trainLoss)\n",
    "        fold_val_losses.append(valLoss)\n",
    "    \n",
    "    meanTrainLoss = np.mean(fold_train_losses)\n",
    "    meanValLoss = np.mean(fold_val_losses)\n",
    "    \n",
    "    return meanTrainLoss, meanValLoss\n",
    "\n",
    "\n",
    "#Doing my actual cross validation for every hyperparameter.\n",
    "results = []\n",
    "\n",
    "for learningRate in allPossibleLearning_rate:\n",
    "    for epoch in allPossibleEpochs:\n",
    "        #error\n",
    "        curMeanTrainLoss , curMeanValLoss = perform_cross_validation(x_train, y_train, epoch, learningRate)\n",
    "        \n",
    "        results.append({'learningRate': learningRate, 'epoch': epoch, 'meanTrainLoss': curMeanTrainLoss, 'meanValLoss': curMeanValLoss})\n",
    "\n",
    "\n",
    "#Print all of the final results for every pair of hyperparameters\n",
    "print(\"\\nFINAL RESULTS:\\n\")\n",
    "for result in results:\n",
    "    print(f\"\\n[Epoch: {result['epoch']}, Learning Rate: {result['learningRate']}\")\n",
    "    print(f\"Average Train Loss: {result['meanTrainLoss']}\")\n",
    "    print(f\"Average Validation Loss: {result['meanValLoss']}\")\n",
    "\n",
    "# Find best hyperparmeter based on our current results\n",
    "# getting best hyperparameters by looking through every hyperparameters object storing its avgTraining and avg validation loss and finding the min.\n",
    "curMinVal = float('inf')\n",
    "curBestObj = None\n",
    "for result in results:\n",
    "    if result[\"meanValLoss\"] < curMinVal:\n",
    "        curMinVal = result[\"meanValLoss\"]\n",
    "        curBestObj = result\n",
    "\n",
    "print(\"\\nBEST HYPEPARAMETERS\")\n",
    "print(f\"Epoch: {curBestObj['epoch']}\")\n",
    "print(f\"Learning Rate: {curBestObj['learningRate']}\")\n",
    "print(f\"Average Validation Loss: {curBestObj['meanValLoss']}\")\n",
    "print(f\"Average Training Loss {curBestObj['meanTrainLoss']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
