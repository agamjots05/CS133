{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOOKING AT EPOCH = 50, LEARNING_RATE = 0.1 \n",
      "\n",
      "Training fold 1/5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learningRate \u001b[38;5;129;01min\u001b[39;00m allPossibleLearning_rate:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m allPossibleEpochs:\n\u001b[0;32m--> 105\u001b[0m         curMeanTrainLoss, curMeanValLoss \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearningRate\u001b[39m\u001b[38;5;124m'\u001b[39m: learningRate, \n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \n\u001b[1;32m    110\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeanTrainLoss\u001b[39m\u001b[38;5;124m'\u001b[39m: curMeanTrainLoss, \n\u001b[1;32m    111\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeanValLoss\u001b[39m\u001b[38;5;124m'\u001b[39m: curMeanValLoss\n\u001b[1;32m    112\u001b[0m         })\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Print all of the final results for every pair of hyperparameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 89\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[0;34m(x_train, y_train, epoch, learningRate)\u001b[0m\n\u001b[1;32m     86\u001b[0m y_val_fold \u001b[38;5;241m=\u001b[39m y_train[val_index]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Find trainingloss and validation loss with given validation and training sets.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m trainLoss, valLoss, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mperform_Logistic_Regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m fold_train_losses\u001b[38;5;241m.\u001b[39mappend(trainLoss)\n\u001b[1;32m     93\u001b[0m fold_val_losses\u001b[38;5;241m.\u001b[39mappend(valLoss)\n",
      "Cell \u001b[0;32mIn[25], line 58\u001b[0m, in \u001b[0;36mperform_Logistic_Regression\u001b[0;34m(xTrain, yTrain, xVal, yVal, epoch, learningRate)\u001b[0m\n\u001b[1;32m     55\u001b[0m yHat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mz))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Calculate gradient according to the formula: f'(x) = (ŷ - y)x\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43myHat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxRow\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Update weights using the update rule: w_new = w_old - η * gradient\u001b[39;00m\n\u001b[1;32m     61\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m-\u001b[39m learningRate \u001b[38;5;241m*\u001b[39m gradient\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load data\n",
    "trainingArr = pd.read_csv('./train_midterm.csv')\n",
    "testArr = pd.read_csv('./test_midterm.csv')\n",
    "\n",
    "x_test = testArr.drop('label', axis=1).values\n",
    "y_test = testArr['label'].values\n",
    "x_train = trainingArr.drop('label', axis=1).values\n",
    "y_train = trainingArr['label'].values\n",
    "\n",
    "# Hyperparameters to tune\n",
    "allPossibleLearning_rate = [0.1, 0.01, 0.001]\n",
    "allPossibleEpochs = [50, 200, 500]\n",
    "\n",
    "def findLossForDataSet(xData, yData, weights):\n",
    "    # Calculate the predicted probabilities\n",
    "    z = np.dot(xData, weights)\n",
    "    yHat = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Calculate binary cross-entropy loss adding small constant (1e-100) to avoid log(0)\n",
    "    loss = -np.mean(yData * np.log(yHat) + (1 - yData) * np.log(1 - yHat))\n",
    "    return loss\n",
    "\n",
    "def perform_Logistic_Regression(xTrain, yTrain, xVal, yVal, epoch, learningRate):\n",
    "    trainingLoss = []\n",
    "    valLoss = []\n",
    "    weights = np.zeros(xTrain.shape[1])\n",
    "    \n",
    "    for ep in range(epoch):\n",
    "        # do stochastic gradient descent\n",
    "        for i in range(len(xTrain)):\n",
    "            xRow = xTrain[i]\n",
    "            y_true = yTrain[i]\n",
    "            \n",
    "            # find your prediction value based on your current weights\n",
    "            oldyHat = np.dot(weights, xRow)\n",
    "\n",
    "            # need to transform your original yHat into value between 0-1 since this is a classification model\n",
    "            yHat = 1 / (1 + np.exp(-oldyHat))\n",
    "            \n",
    "            gradient = (yHat - y_true) * xRow\n",
    "            \n",
    "            # updatate weights after each row, property of SGD\n",
    "            weights = weights - learningRate * gradient\n",
    "        \n",
    "        # find losses at the end of each epoch\n",
    "        train_loss = findLossForDataSet(xTrain, yTrain, weights)\n",
    "        trainingLoss.append(train_loss)\n",
    "\n",
    "        # only finding validation loss if there is a validation set that is being provided.\n",
    "        if xVal != None:\n",
    "            val_loss = findLossForDataSet(xVal, yVal, weights)\n",
    "            valLoss.append(val_loss)\n",
    "            return trainingLoss[-1], None, trainingLoss, None, weights\n",
    "        \n",
    "    \n",
    "    #returning both the last validation and training loss as well as the entire trainingLoss and valLoss history depending on whether the cross validation method is calling it, or in the end where you need to do the final training.\n",
    "    return trainingLoss[-1], valLoss[-1], trainingLoss, valLoss, weights\n",
    "\n",
    "\n",
    "def perform_cross_validation(x_train, y_train, epoch, learningRate):\n",
    "    print(f\"\\nLOOKING AT EPOCH = {epoch}, LEARNING_RATE = {learningRate} \\n\")\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    #splitting data into 5 blocks and using validation and training data to find averageValidationLoss and avgTrainingLoss for each hyperparameter\n",
    "    for (train_index, val_index) in enumerate(kf.split(x_train)):\n",
    "        x_train_fold = x_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        x_val_fold = x_train[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Find trainingloss and validation loss with given validation and training sets.\n",
    "        trainLoss, valLoss, ignore, ignore, ignore = perform_Logistic_Regression(x_train_fold, y_train_fold, x_val_fold, y_val_fold, epoch, learningRate)\n",
    "        fold_train_losses.append(trainLoss)\n",
    "        fold_val_losses.append(valLoss)\n",
    "    \n",
    "    meanTrainLoss = np.mean(fold_train_losses)\n",
    "    meanValLoss = np.mean(fold_val_losses)\n",
    "    \n",
    "    return meanTrainLoss, meanValLoss\n",
    "\n",
    "# do cross-validation for all hyperparameter combinations\n",
    "results = []\n",
    "\n",
    "for learningRate in allPossibleLearning_rate:\n",
    "    for epoch in allPossibleEpochs:\n",
    "        curMeanTrainLoss, curMeanValLoss = perform_cross_validation(x_train, y_train, epoch, learningRate)\n",
    "        \n",
    "        results.append({'learningRate': learningRate, 'epoch': epoch, 'meanTrainLoss': curMeanTrainLoss, 'meanValLoss': curMeanValLoss})\n",
    "\n",
    "print(\"\\nFINAL RESULTS:\\n\")\n",
    "for result in results:\n",
    "    print(f\"\\n[Epoch: {result['epoch']}, Learning Rate: {result['learningRate']}\")\n",
    "    print(f\"Average Train Loss: {result['meanTrainLoss']}\")\n",
    "    print(f\"Average Validation Loss: {result['meanValLoss']}\")\n",
    "\n",
    "# Find best hyperparameter based on validation loss\n",
    "curMinVal = float('inf')\n",
    "curBestObj = None\n",
    "for result in results:\n",
    "    if result[\"meanValLoss\"] < curMinVal:\n",
    "        curMinVal = result[\"meanValLoss\"]\n",
    "        curBestObj = result\n",
    "\n",
    "print(\"\\nBEST HYPERPARAMETERS\")\n",
    "print(f\"Epoch: {curBestObj['epoch']}\")\n",
    "print(f\"Learning Rate: {curBestObj['learningRate']}\")\n",
    "print(f\"Average Validation Loss: {curBestObj['meanValLoss']}\")\n",
    "print(f\"Average Training Loss {curBestObj['meanTrainLoss']}\")\n",
    "\n",
    "#doing final training on entire dataset and use the best hyperparameter you found with cross fold validation\n",
    "print(\"\\nTraining final model with best hyperparameters on full dataset\")\n",
    "#setting both the xVal and yVal set as 'None' since we're not finding the validation loss in this final training.\n",
    "ignore, ignore, train_loss_history, bestWeights = perform_Logistic_Regression(x_train, y_train, None, None, curBestObj['epoch'], curBestObj['learningRate'])\n",
    "\n",
    "\n",
    "#finding test loss after all training using best weights\n",
    "testLoss = findLossForDataSet(xTrain, yTrain)\n",
    "print(f\"\\n Final Test Loss = {testLoss}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(y=testLoss, color=red label='Test Loss')\n",
    "# plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.title(f'Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
